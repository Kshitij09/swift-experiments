{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var logits = Tensor<Float32>(randomNormal: [10,5], mean: Tensor(0), standardDeviation: Tensor(4))\n",
    "var targets = Tensor<Int32>(randomUniform: [10], lowerBound: Tensor(0), upperBound: Tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -7.5606837,  -4.1594152,  -1.4371437,  0.94998556,  -2.1296244],\n",
      " [   5.460545,   0.9486486,   5.6935763,   0.7171767,   3.3408916],\n",
      " [  -5.106072,  -1.2702762,  -1.3153108,   -4.035973,  -5.5275283],\n",
      " [0.109969355,  -3.3563774,  -11.882913,    5.777429,  -1.5817682],\n",
      " [ -3.3135924,    0.683405,  -4.6655736,   1.4658132,   4.1020246],\n",
      " [   3.543782,    8.338613,   3.8994873,  0.21737371,   1.3145567],\n",
      " [  1.1342677,   2.9519236,    2.331409,   -5.044607,   -1.913505],\n",
      " [   2.484174,   1.2623852,   2.4720688,   -5.913093,   -6.056792],\n",
      " [   -8.37429,   -4.443255,   -5.749257,  -1.6235662,   5.7547674],\n",
      " [  3.2606332,   2.0625463,   -6.767269,  -1.5690302,  -1.1145483]]\n",
      "[1, 4, 0, 2, 1, 1, 3, 1, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0]]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let oneHotLogits = Tensor<Float32>(oneHotAtIndices: logits.argmax(squeezingAxis: 1),\n",
    "                                   depth: logits.shape[1]); oneHotLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: [-7.5606837, -4.1594152, -1.4371437, 0.94998556, -2.1296244] \n",
      "Label:1 \n",
      "Loss:5.7067485 \n",
      "Gradients:[-1.7595223e-05,    0.099472106,   -0.008031833,    -0.08740408,   -0.004018594]\n"
     ]
    }
   ],
   "source": [
    "let (loss, gradients) = valueWithGradient(at: logits) {logits in \n",
    "                             softmaxCrossEntropy(logits: logits, labels: targets)\n",
    "                        }\n",
    "print(\"logits: \\(logits[0]) \\nLabel:\\(targets[0]) \\nLoss:\\(loss) \\nGradients:\\(-gradients[0])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: [0.0, 0.0, 0.0, 1.0, 0.0] \n",
      "Label:1 \n",
      "Loss:1.8048325 \n",
      "Gradients:[-0.01488476,  0.08511525, -0.01488476, -0.04046097, -0.01488476]\n"
     ]
    }
   ],
   "source": [
    "let (loss, gradients) = valueWithGradient(at: oneHotLogits) {logits in \n",
    "                             softmaxCrossEntropy(logits: logits, labels: targets)\n",
    "                        }\n",
    "print(\"logits: \\(oneHotLogits[0]) \\nLabel:\\(targets[0]) \\nLoss:\\(loss) \\nGradients:\\(-gradients[0])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While the gradients work fine in both the cases, raw values result in significantly higher loss as compared to loss with one-hot encoded logits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding ways to implement `ignore_index` for crossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "var logits = Tensor<Float32>(randomNormal: [10,5], mean: Tensor(0), standardDeviation: Tensor(4))\n",
    "var targets = Tensor<Int32>(randomUniform: [10], lowerBound: Tensor(0), upperBound: Tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -7.816966,   1.6490363,  -2.5067804,   1.1694168,   0.8058538],\n",
      " [ -1.4318359,  0.91529536,   3.3752975,  -0.6105987,  -3.2968643],\n",
      " [  11.477498,  -0.2612766, -0.92973423,   -5.894342,  -3.5515406],\n",
      " [  2.6089673,   -3.037576,  -4.1986403,     3.46875,   -5.185842],\n",
      " [  4.8170567,    3.301774,    1.725311,   3.6389813,  -1.3453578],\n",
      " [ -2.3146663,  -6.5037622,  -2.8066723,    5.053336,  0.74045706],\n",
      " [  1.9974203,   -2.865681,  -3.5518906, 0.030817203,  -2.1184027],\n",
      " [  -2.112357,   -2.725534,     2.77681,   1.4542646,   3.3758972],\n",
      " [  5.8977785,   3.1237755,     -6.5468,    6.299514,    4.218277],\n",
      " [ -1.7487556,   -5.027978,   3.2570636,  0.73814887,   1.3552209]]\n",
      "[1, 1, 4, 4, 4, 4, 1, 0, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked up for cleaner solution similar to pyTorch, but unfortunately, there isn't any. (There was one (issue)[https://github.com/keras-team/keras/issues/6118] on keras repo which didn't receive any positive feedback, also, this is not supported by tensorflow_ops at first place)\n",
    "\n",
    "\n",
    "Let me endeavour doing it contrived way. I've `logits` with shape (10,5) and `targets` with shape(initially being sparse) which I'm one-hot encoding to deal with indices. The shape here indicates I've 10 examples with 5 classes. Now, consider I want to ignore the index `2` from the loss, we need to mask our inputs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "public func debugGradients<T: TensorFlowFloatingPoint>(logits: Tensor<T>,targets: Tensor<T>) {\n",
    "    let (loss, gradients) = valueWithGradient(at: logits) { logits in \n",
    "                                softmaxCrossEntropy(logits: logits, probabilities: targets)\n",
    "                            }\n",
    "    print(\"logits: \\(logits[0]) \\nLabel:\\(targets[0]) \\nLoss:\\(loss) \\nGradients:\\(-gradients[0])\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "var oneHotTargets = Tensor<Float32>(oneHotAtIndices: targets, depth: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: [ -7.816966,  1.6490363, -2.5067804,  1.1694168,  0.8058538] \n",
      "Label:[0.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Loss:5.265192 \n",
      "Gradients:[-3.7499424e-06,    0.051576387, -0.00075894274,   -0.029975135,    -0.02083856]\n"
     ]
    }
   ],
   "source": [
    "debugGradients(logits: logits, targets: oneHotTargets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ignoring index `2` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices after exclude: [0, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "var ids = Tensor(rangeFrom: 0, to: 5, stride: 1)\n",
    "let indices = _Raw.where_(_Raw.notEqual(ids, Tensor(2))).squeezingShape(at: 1)\n",
    "print(\"Indices after exclude: \\(indices)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: [-7.816966, 1.6490363, 1.1694168, 0.8058538] \n",
      "Label:[0.0, 1.0, 0.0, 0.0] \n",
      "Loss:4.828433 \n",
      "Gradients:[-3.7786199e-06,    0.051206063,   -0.030204369,   -0.020997923]\n"
     ]
    }
   ],
   "source": [
    "let maskedLogits = logits.gathering(atIndices: indices, alongAxis: 1)\n",
    "let maskedTargets = oneHotTargets.gathering(atIndices: indices, alongAxis: 1)\n",
    "\n",
    "debugGradients(logits: maskedLogits, targets: maskedTargets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the gradients, we got rid of grad at idx=2, affecting total loss as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
